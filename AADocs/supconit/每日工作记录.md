## 2018-09-21
已经改进之处：
1. 添加资源，添加资源类型（后缀）一项 
2. glue_shell 改为shell       
3. update的时间都是正确
4. 添加任务后，可以在更新任务时修改jobGroup（执行器），并成功发送到该机器执行任务
在任务代码页触发一次任务
5. 把....index model改成接口的形式
6. 添加根据资源类型查找资源
7. 任务日志管理页可以直接跳转到对应的历史日志页 getLogsByJobId 要测试

[未测通的接口]
getXxlExecutorHosts

## 2018-09-04
[待完成任务]
Impala-hbase
Presto 关联查询 先调研 优先考虑这个 如果可以优先做
GreenPlum 其实只是一个MPP数据库 完整sql支持 pgsql
Hawq
调研这三个 出调研报告 数栖用的是Presto 看哪个适合我们
大数据逻辑上的即席查询 大数据的查询引擎
例如Mysql和es的联合查询 多表join 看能否解决跨不同类型数据库的查询
1.架构区别
2.适用场景
3.条件限制
4.资源消耗

## 2018-8-31
[待完成任务]  
1.JobManagerController测试
2.添加新建资源，在脚本中可以添加资源依赖
3.脚本设置运行入参

## 2018-08-09
 晚上开会提到的
 [待完成任务-Notebook部分的修改]
 
 1.notebook放到开发中心里
 2.notebook的运行结果，例如图片，可以让用户下载。后期也许要做成可以让用户直接导入到excel表中。
 3.notebook页面可以查看hdfs上都有哪些文件，以及对应的文件内容（例如一个数据文件中数据的格式），让用户在分析代码时可用
 

## 2018-08-07
[待完成的任务]

1.Notebook接口的测试
2.定时任务 页面设计
3.quartz spring的整合
4.quartz的框架

[定时任务部分需要完成的点]

1.用户可以自己上传资源文件(jar)
用户上传到一台服务器上，通过quartz调度到不同的服务器
2.用户新建脚本，保存脚本(shell或python)
利用Jsch使用ssh协议连接到远程shell执行脚本
看在quartz下是否能完成这一点
3.用户运行脚本，调度脚本
4.脚本运行调度后的日志返回
5.脚本的入参可以自己设定

## 2018-08-01
大数据平台2.0有如下未解决的问题：
1、zeppelin在同一个解释器中，必须先执行完上一个任务，才能执行下一个。
看一下如何启动多个解释器。（高可用形式）
或者干脆给用户信息：现在前面的一个任务是什么，现在运行到了哪一个。（任务排队形式）
2、zeppelin可以是集群形式吗？
3、Quartz任务调度，集群分布式的形式。
提交给Quartz后，让它自动调度到装了Quartz的集群上执行。
但是我们需要知道它把任务调度到了哪个节点上，怎么获取这个任务的运行结果信息。需要手动吗？
取后返回给用户。
对比一下celery和数栖平台的实现。
4、项目的用户管理。
一个平台：一个项目
一个平台：多个项目
5、zeppelin暂时先做成和数栖平台那样，新建一个文件，就是新建一个Notebook的形式。
看一下怎么获取图片分析结果的这个图片。

## 2018-07-24
1. 在zeppelin上测试用scala进行数据处理的代码
2. 解决用ambari安装zeppelin的以下问题：
√1、自动生成的zeppelin-env脚本有换行问题。
   去掉脚本中的文本问题后，手动启动可以启。
   
   需要写出到新文件的配置文件：
   从zeppelin-config.xml写出到zeppelin-site.xml
   
   从zeppelin-config.xml的env_content这个key对应的值写出到zeppelin-env.sh
   
   或者可以像柴泽照那样从模板文件中直接写。
   
√ 2、安装的zeppelin还是需要登录
   应该是因为，将shiro.ini.template文件写出到了shiro.ini文件了。如果不想要登录用户名和密码的话，这个文件可以不写出。
3、使用%python，运行一些代码会报错。（这是Linux的python版本的问题。不知该如何解决。）
   ImportError: No module named pandas
4、使用spark已经没有问题。

## 2018-07-23
1. zeppelin 多节点模式
2. zeppelin能否接入tensorflow
3. Azkaban..Ozzie
4. 数据库，消息中间件
作业流调度
数据治理
用户管理

## 2018-07-16
√1.测试zeppelin notebook部署接口
√2.部署hadoop和notebook到服务器
3.zeppelin 0.8.0 使用ambari安装
4.spark 2.1.1使用ambrai安装
5.ambari定制zeppelin 0.8.0版本
  zeppelin 0.8.0的spark调试还有些问题，jar包冲突，未解决
6.ambari安装的hive无法使用，报错
#### 主要存在问题
ambari安装的zeppelin：
1.需要登录才能用rest接口
2.不能使用python

## 2018-07-06
1.存在问题：zeppelin版本与spark版本冲突
√用zeppelin 0.8.0的版本自己安装
--安装很容易
√解决restful接口需要登录访问的问题(如果restful接口不行，可能需要看源码，修改源码。)
--自己安装的版本没有这个问题
√是否可以运行Python程序
--可以
√如何把运行后的图表返回（可能需要看源码）
--直接用iframe的形式返回
ambari定制zeppelin 0.8.0版本
zeppelin 0.8.0的spark调试还有些问题，jar包冲突，未解决

2.jupterNotebook的安装
如何集成到Java中。或者用Iframe的方式嵌入到页面？
调用restful api？

√3.hdfs改为hadoop
如何获取mapreduce任务的log日志？
--存在hdfs上，czz安装的135机器的hadoop JobhistoryServer可以使用，但是ambari安装的机器10.10.99.37无法使用hive，还不能测试JobhistoryServer
ambari安装的hive无法使用，报错


## 2018-07-05 
将hdfs页面改为hadoop页面
要获取以下参数：
id
User=hadoop
Name
applicationTypes=MAPREDUCE
starttime 开始时间
finishtime 完成时间
elapsedTime 消耗时间（毫秒，转化为秒要除以1000）
state 完成状态
finalstatus 最终状态
Diagnostics 诊断


## 2018-07-04

1、hbase 等工具的版本选取问题，不同的版本功能有所不同。不要到后期来改。
了解一下hbase的使用场景，什么数据适合使用hbase。

2、hadoop的页面（由Hdfs修改而来）
hadoop页面包括任务运行和hdfs两块
其中任务运行：包括提交的MapReduce信息，日志、任务的执行情况。
（sqoop, spark提交mapreduce任务后，只能从hadoop的页面上才能查看任务的详细信息）
可以只显示前面多少条（可能会很多）

3、jupter notebook集成

----------------------------------------


kafka broker概念
topic


streaming读取Kafka数据
sparkstreaming java的使用


hdfs的文件操作，工具类是否可用。
spark 转化操作


1、显示集群中目前正在运行的所有applications，历史应用就不需要了。

2、每个application中显示前10-20条job，假如有哪条job是失败的，可以显示出来。

3、最好可以显示当前（在某个运行时间段内）已经处理了多少数据
Sparkstreaming rest api无法访问，是因为spark 2.2.0版本开始支持这个rest api，ambari自带的spark 2.1.x还不支持


<property>
      <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
      <value>0.2</value>
    </property>

改为0.5，无用。


用yarn跑多个spark应用，当一个应用已经在running时，提交另外一个应用始终是accepted状态（未运行）。后续需要仔细看下这部分，yarn的应用间资源调度问题。


--疑问点

hbase 原生页面可以查看表数据吗？
hbase rest api
hue中操作hbase 是用到了hive on hbase？
phonix 操作hbase：用phonix创建hbase表后，用Hbase原生api访问指定列时，会有点问题。





spring soa hcsoa
hiveserver2和metastore



mysql用户名密码
root
supconit

1.为ambari建立数据库
# yum install mysql-connector-java
#  mysql -u root -p
USER 'ambari'@'%' IDENTIFIED BY 'supconit'; 
GRANT ALL PRIVILEGES ON *.* TO 'ambari'@'%';
CREATE USER 'ambari'@'localhost' IDENTIFIED BY 'supconit';
GRANT ALL PRIVILEGES ON *.* TO 'ambari'@'localhost';


## 2018.6.12
应用信息的状态有哪几种。
ACCEPTED，RUNNING，FINISHED，KILLED
待分配，运行中，已结束，被终止
有accepted吗？（下载spark源码。）

列出Hdfs页面需要的参数，给ui。
用ambari安装Hbase。
熟悉hbase表结构，命令行操作命令，rest api访问。


hadoop mapreduce的日志在哪里。

## 2018.6.11
修正spark代码的规范性。
给前端列出流式处理-spark接口说明。
部署spark代码。
解决前端无法访问到spark后端接口的问题。
学习postman的使用。


## 2018.6.8
列出之后任务的截止时间

显示上限 不要直接在代码里面写数字
当前作业信息 要改成jsonarray转化javabean的形式


√ 流数据源信息（一个还是多个？）
√ 当前批次信息
√ 完成批次信息
完成作业信息



## 2018.6.7
确定spark页面需要展示的参数信息，写后端提供数据的api。
√时间的转换

√ 集群概览
√ 应用列表
√ 应用信息
√ 当前作业信息

√ 流统计信息





## 2018.6.6
1、确定第二版spark页面需要的参数信息，与UI对接。
2、如何将XML格式json转化为javabean


## 2018.6.5
根据第一版spark页面，做以下调研：

1、显示集群中目前正在运行的所有applications，历史应用就不需要了。
2、每个application中显示前10-20条job
3、最好可以显示当前（在某个运行时间段内）已经处理了多少数据
Sparkstreaming rest api无法访问，是因为spark 2.2.0版本开始支持这个rest api，ambari自带的spark 2.1.x还不支持


## 2018.6.4
使用java 操作spark
java 操作sparkstreaming从socket读取数据的demo
spark中的transformation操作和action操作。
用java 操作kafka producer发送消息demo和consumer接收消息demo（未完成）


## 2018.5.17

静态普通方法匹配切面
动态切面
流程控制切面
复合切面

自动创建代理：
基于bean配置名规则的自动代理创建
基于advisor匹配机制的自动代理创建
基于bean中aspjectJ注解标签的自动代理创建

chapter 7 基于@AspectJ和Schema的AOP
djk注解
如何通过@AspectJ定义切面
切点函数
绑定连接点参数
基于Schema配置定义切面

在spring中使用myBatis

Chapter 8 Spring对DAO的支持
Chapter 9 Spring的事务管理

【未看内容】
ThreadLocal
struct框架
storm
mybatis



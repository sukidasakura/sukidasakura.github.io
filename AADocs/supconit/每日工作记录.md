## 2018-07-06
1.存在问题：zeppelin版本与spark版本冲突
√用zeppelin 0.8.0的版本自己安装
--安装很容易
√解决restful接口需要登录访问的问题
--自己安装的版本没有这个问题
√是否可以运行Python程序
--可以
如何把运行后的图表返回（可能需要看源码）
如果restful接口不行，可能需要看源码，修改源码。
ambari定制zeppelin 0.8.0版本

2.jupterNotebook的安装
如何集成到Java中。或者用Iframe的方式嵌入到页面？
调用restful api？

3.hdfs改为hadoop
如何获取mapreduce任务的log日志并返回给前端


## 2018-07-05 
将hdfs页面改为hadoop页面
要获取以下参数：
id
User=hadoop
Name
applicationTypes=MAPREDUCE
starttime 开始时间
finishtime 完成时间
elapsedTime 消耗时间（毫秒，转化为秒要除以1000）
state 完成状态
finalstatus 最终状态
Diagnostics 诊断


## 2018-07-04

1、hbase 等工具的版本选取问题，不同的版本功能有所不同。不要到后期来改。
了解一下hbase的使用场景，什么数据适合使用hbase。

2、hadoop的页面（由Hdfs修改而来）
hadoop页面包括任务运行和hdfs两块
其中任务运行：包括提交的MapReduce信息，日志、任务的执行情况。
（sqoop, spark提交mapreduce任务后，只能从hadoop的页面上才能查看任务的详细信息）
可以只显示前面多少条（可能会很多）

3、jupter notebook集成

----------------------------------------


kafka broker概念
topic


streaming读取Kafka数据
sparkstreaming java的使用


hdfs的文件操作，工具类是否可用。
spark 转化操作


1、显示集群中目前正在运行的所有applications，历史应用就不需要了。

2、每个application中显示前10-20条job，假如有哪条job是失败的，可以显示出来。

3、最好可以显示当前（在某个运行时间段内）已经处理了多少数据
Sparkstreaming rest api无法访问，是因为spark 2.2.0版本开始支持这个rest api，ambari自带的spark 2.1.x还不支持


<property>
      <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
      <value>0.2</value>
    </property>

改为0.5，无用。


用yarn跑多个spark应用，当一个应用已经在running时，提交另外一个应用始终是accepted状态（未运行）。后续需要仔细看下这部分，yarn的应用间资源调度问题。


--疑问点

hbase 原生页面可以查看表数据吗？
hbase rest api
hue中操作hbase 是用到了hive on hbase？
phonix 操作hbase：用phonix创建hbase表后，用Hbase原生api访问指定列时，会有点问题。





spring soa hcsoa
hiveserver2和metastore



mysql用户名密码
root
supconit

1.为ambari建立数据库
# yum install mysql-connector-java
#  mysql -u root -p
USER 'ambari'@'%' IDENTIFIED BY 'supconit'; 
GRANT ALL PRIVILEGES ON *.* TO 'ambari'@'%';
CREATE USER 'ambari'@'localhost' IDENTIFIED BY 'supconit';
GRANT ALL PRIVILEGES ON *.* TO 'ambari'@'localhost';


## 2018.6.12
应用信息的状态有哪几种。
ACCEPTED，RUNNING，FINISHED，KILLED
待分配，运行中，已结束，被终止
有accepted吗？（下载spark源码。）

列出Hdfs页面需要的参数，给ui。
用ambari安装Hbase。
熟悉hbase表结构，命令行操作命令，rest api访问。


hadoop mapreduce的日志在哪里。

## 2018.6.11
修正spark代码的规范性。
给前端列出流式处理-spark接口说明。
部署spark代码。
解决前端无法访问到spark后端接口的问题。
学习postman的使用。


## 2018.6.8
列出之后任务的截止时间

显示上限 不要直接在代码里面写数字
当前作业信息 要改成jsonarray转化javabean的形式


√ 流数据源信息（一个还是多个？）
√ 当前批次信息
√ 完成批次信息
完成作业信息



## 2018.6.7
确定spark页面需要展示的参数信息，写后端提供数据的api。
√时间的转换

√ 集群概览
√ 应用列表
√ 应用信息
√ 当前作业信息

√ 流统计信息





## 2018.6.6
1、确定第二版spark页面需要的参数信息，与UI对接。
2、如何将XML格式json转化为javabean


## 2018.6.5
根据第一版spark页面，做以下调研：

1、显示集群中目前正在运行的所有applications，历史应用就不需要了。
2、每个application中显示前10-20条job
3、最好可以显示当前（在某个运行时间段内）已经处理了多少数据
Sparkstreaming rest api无法访问，是因为spark 2.2.0版本开始支持这个rest api，ambari自带的spark 2.1.x还不支持


## 2018.6.4
使用java 操作spark
java 操作sparkstreaming从socket读取数据的demo
spark中的transformation操作和action操作。
用java 操作kafka producer发送消息demo和consumer接收消息demo（未完成）


## 2018.5.17

静态普通方法匹配切面
动态切面
流程控制切面
复合切面

自动创建代理：
基于bean配置名规则的自动代理创建
基于advisor匹配机制的自动代理创建
基于bean中aspjectJ注解标签的自动代理创建

chapter 7 基于@AspectJ和Schema的AOP
djk注解
如何通过@AspectJ定义切面
切点函数
绑定连接点参数
基于Schema配置定义切面

在spring中使用myBatis

Chapter 8 Spring对DAO的支持
Chapter 9 Spring的事务管理

【未看内容】
ThreadLocal
struct框架
storm
mybatis



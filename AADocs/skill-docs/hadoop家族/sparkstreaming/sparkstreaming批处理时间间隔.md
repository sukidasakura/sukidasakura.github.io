1.SparkStreaming的批处理时间间隔

很容易陷入的一个误区就是，以为时间间隔30秒就是每30秒从kafka读取一次。其实不然，可以理解为数据向水流一样源源不断的从kafka中读取出来（只要定义了DStream，Spark程序就会将接收器在各个节点上启动，接收器会以独立线程的方式源源不断的接受数据），每积累30秒钟的数据作为一个RDD供进行一次处理。